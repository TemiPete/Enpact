---
title: "Reproducing the Enpact data and results"
author: "Temi"
description: "This notebook contains steps and codes to reproduce, as much as possible, the Enpact paper results"
date: 'Wed Sep 11 2024'
html:
    self-contained: true
    code-background: true
fig-format: svg
---

```{r}
knitr::opts_chunk$set(eval = F, echo = TRUE, message = TRUE, warning = TRUE, cache = TRUE)
```

```{r}
setwd('/project/haky/users/temi/projects/Enpact/notebooks')
```

```{r}
library(data.table)
library(glue)
library(dplyr)
library(RSQLite)
library(rtracklayer)
library(GenomicRanges)
library(magrittr)
library(yaml)
library(fpeek)
library(purrr)
```

```{r}
# tdate <- '2023-08-17' # modified
tdate <- '2024-04-17'

base_dir <- '/project2/haky/temi/projects/TFXcan/baca_cwas'
data_dir <- '/project/haky/users/temi/projects/Enpact'

output_dir <- glue('{base_dir}/data')
files_dir <- glue('{data_dir}/files')
if(!(dir.exists(files_dir))){
    dir.create(files_dir, recursive = T)
}
```

# Introduction
In this notebook, I detail steps to re-create some of the data/results in the Enpact paper. I have set this notebook not to run when rendering. So, it will show you the code and markdown text, but not run the code.

# Train Enpact models

## Prepare the metadata and files

```{r}
homerdb <- data.table::fread(file.path(data_dir, 'metadata', 'motifTable.txt'), header = T) %>%
    dplyr::filter(!grepl(',', `Gene Symbol`), !`Gene Symbol` %in% c( "-", "?")) %>%
    dplyr::select(Filename, Consensus, symbol=`Gene Symbol`) %>%
    dplyr::filter(symbol %in% c('AR', 'FOXA1', 'GATA2', 'HOXB13'))
homerdb
```

```{r}
mt <- data.table::fread(file.path(data_dir, 'metadata', 'human_factor_full_QC.txt')) %>%
    dplyr::filter(Tissue_type != 'None', !is.na(PeaksUnionDHSRatio), FRiP > 0.01) %>%
    dplyr::filter(!grepl('-', Factor, fixed = T)) %>%
    dplyr::filter(Factor %in% homerdb$symbol)

idt <- mt %>%
    dplyr::filter(Factor %in% c('AR', 'FOXA1', 'GATA2', 'HOXB13'), Tissue_type == 'Prostate') %>%
    dplyr::group_by(Factor, Tissue_type) %>%
    dplyr::group_split() 
```

```{r}
ldt <- list()
for(i in seq_along(idt)){
    fdt <- idt[[i]]
    tissuename <- unique(fdt$Tissue_type)
    tissuename <- gsub(' ', '', tissuename)
    tfname <- unique(fdt$Factor)
    dcids <- fdt$DCid

    if(!tfname %in% names(ldt)){
        ldt[[as.name(tfname)]] <- list()
        ldt[[as.name(tfname)]][['peakFiles']] <- list()
        ldt[[as.name(tfname)]][['peakFiles']][[as.name(tissuename)]] <-  paste0(dcids, '_sort_peaks.narrowPeak.bed')
        
    } else {
        ldt[[as.name(tfname)]][['peakFiles']][[as.name(tissuename)]] <-  paste0(dcids, '_sort_peaks.narrowPeak.bed')
    }
}
```

```{r}
factors_motifs <- homerdb %>%
    dplyr::group_by(symbol) %>%
    dplyr::group_split()
ndt <- c()
mdt <- lapply(factors_motifs, function(edt){
    res <- list()
    res[['motifFiles']] <- edt$Filename
    ndt <<- append(ndt, unique(edt$symbol))
    return(res)
})
names(mdt) <- ndt
```

```{r}
common_names <- intersect(names(ldt), names(mdt))
peakslist <- ldt[common_names]
motifslist <- mdt[common_names]
enpact_models_config <- mapply(c, peakslist, motifslist)
```

```{r}
# Modified from: https://stackoverflow.com/questions/74655073/how-to-effectively-join-two-lists-elementwise-by-element-name
cat_lists <- function(list1, list2) {  
  keys <- unique(c(names(list1), names(list2)))
  map2(list1[keys], list2[keys], c) |>
    set_names(keys)  

}

enlist <- purrr::reduce(list(motifslist, peakslist), cat_lists)
```

-- Filter for the following TFs: AR, FOXA1, HOXB13, and GATA2 in prostate tissues

```{r}
tfs_ex <- enlist[c('AR', 'FOXA1', 'HOXB13', 'GATA2')]
tfs_ex <- base::Filter(Negate(is.null), tfs_ex)

yaml::write_yaml(tfs_ex, file = file.path(data_dir, 'metadata', 'enpact_models_to_train.yaml'))

# prepare the metadata 
mtdt <- sapply(gsub('.peakFiles.', '_', names(rapply(peakslist, function(x) head(x, 1)))), base::strsplit, '_') |> unname() %>% do.call('rbind', .) %>% as.data.frame()
colnames(mtdt) <- c('assay', 'context')

data.table::fwrite(mtdt, file = file.path(data_dir, 'metadata', 'enpact_models_to_train.tsv'), sep = '\t', row.names =F, col.names =T, quote = F)
```

These files created and saved are used to train the DL-based Enpact models.

Steps required to train Enpact models are [here](https://github.com/hakyimlab/TFPred-snakemake). There is a minimal example to follow. As well as the necessary scripts to run the training for AR, FOXA1, HOXB13, and GATA2 in prostate tissues.

# Prepare Baca's CWAS models in sqlite and create the necessary databases
-- the CWAS weights for AR
```{r}
transcription_factor <- 'AR'
ar_zip <- '/project2/haky/Data/baca_cwas/cwas_weights/AR.zip'
print(file.exists(ar_zip))
```

-- First unzip the file
```{r}

if(!dir.exists(glue('{output_dir}/{transcription_factor}'))){
    file_names <- unzip(ar_zip, list=T)$Name
    files_to_read <- grep(pattern='^\\bAR\\b.*\\bRDat\\b$', x=file_names, value=T)
    files_to_read[1:5]

    # unzip the file
    zip::unzip(ar_zip, files=files_to_read, exdir=output_dir)

} 

ar_files <- list.files(glue('{output_dir}/{transcription_factor}'))
ar_files_locus <- sapply(strsplit(x=ar_files, split='\\.'), getElement, 1)
ar_files_locus[1:5]
```

-- read weights; Next, read the files `.wgt` files
```{r}
out <- purrr::map(.x=seq_along(ar_files_locus), .f=function(i){
    locus <- ar_files_locus[i]
    #print(file.exists(glue('{output_dir}/{transcription_factor}/{locus}.wgt.RDat')))
    rdt <- new.env(parent = emptyenv())
    load(glue('{output_dir}/{transcription_factor}/{locus}.wgt.RDat'), envir=rdt)
    wgts <- as.data.frame(rdt$wgt.matrix) %>% 
        tibble::rownames_to_column('snp_id') %>% 
        dplyr::mutate(locus=locus)
    
    snp_info <- rdt$snps %>% 
        as.data.frame() %>% 
        dplyr::select(all_of(c('V1', 'V3', 'V2', 'V4', 'V5'))) 

    colnames(snp_info) <- c('chr', 'snp_id', 'position', 'a1', 'a2')

    dt <- base::merge(wgts, snp_info, by='snp_id') %>% 
        dplyr::relocate(all_of(c('locus', 'chr', 'position', 'a1', 'a2')), .after=snp_id)
    return(dt)
}, .progress=T)

cwas_db <- do.call('rbind', out)

dim(cwas_db) ; cwas_db[1:5, ]
```

-- write out the weights
```{r}
data.table::fwrite(cwas_db, file=glue('{files_dir}/{transcription_factor}_baca_cwas_weights.hg19.{tdate}.txt.gz'), col.names=T, row.names=F, quote=F, compress='gzip',sep = '\t')
cwas_db <- data.table::fread(glue('{files_dir}/{transcription_factor}_baca_cwas_weights.hg19.{tdate}.txt.gz'))
```

-- read and write out the extras

```{r}
out <- purrr::map(.x=seq_along(ar_files_locus), .f=function(i){
    locus <- ar_files_locus[i]
    #print(file.exists(glue('{output_dir}/{transcription_factor}/{locus}.wgt.RDat')))
    rdt <- new.env(parent = emptyenv())
    load(glue('{output_dir}/{transcription_factor}/{locus}.wgt.RDat'), envir=rdt)

    cv_perf <- rbind(rdt$cv.performance['pval', ], rdt$cv.performance['rsq', ]) %>%
        as.data.frame() %>%
        dplyr::mutate(measure = c('pval', 'rsq'), locus) %>%
        dplyr::relocate(locus, measure) %>%
        dplyr::mutate(locus = locus, transcription_factor = transcription_factor, n_snps_in_window = rdt$N.tot, n.snps.in.model = rdt$N.as)

    return(cv_perf)
}, .progress=T)

cvperf_dt <- do.call('rbind', out)
dim(cvperf_dt) ; cvperf_dt[1:5, ]
```

```{r}
data.table::fwrite(cvperf_dt, file=glue('{files_dir}/{transcription_factor}_baca_cwas_extras.hg19.{tdate}.txt.gz'), col.names=T, row.names=F, quote=F, compress='gzip',sep = '\t')
```

-- map the loci to hg38 from hg19

### Bed mappings i.e. liftover the arbs' coordinates from hg19 to hg38

```{r}
#write out the bed files
cvperf_dt %>%
    tidyr::separate_wider_delim(col = locus, names = c('chr', 'start', 'end'), delim = stringr::regex(':|-')) %>%
    dplyr::mutate(across(c(start, end), as.numeric)) %>%
    dplyr::select(chr, start, end) %>%
    dplyr::distinct() %>%
    dplyr::mutate(id = 1:nrow(.)) %>%
    data.table::fwrite(file=glue('{data_dir}/files/baca_cwas_loci_hg19.bed'), col.names=F, row.names=F, quote=F, sep='\t')
```

```{r}
# lift over these files
# download the liftover command : https://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/liftOver
if(!file.exists(file.path(data_dir, 'software', 'liftOver'))){
    download.file('https://hgdownload.cse.ucsc.edu/admin/exe/linux.x86_64/liftOver', destfile = file.path(data_dir, 'software', 'liftOver'))
}
```

```{r}
liftover_sh <-  file.path(data_dir, 'src', 'liftoverSingleBed.sbatch')
liftover_exe <- file.path(data_dir, 'software', 'liftOver')
input_bed <- glue('{data_dir}/files/baca_cwas_loci_hg19.bed')
chain_file <- file.path(data_dir, 'helpers', 'hg19ToHg38.over.chain.gz')
output_bed <- glue('{data_dir}/files/baca_cwas_loci_hg38.bed')
unmapped_bed <- glue('{data_dir}/files/baca_cwas_loci_hg19.unmapped.bed')

file.exists(liftover_sh) ; file.exists(liftover_exe) ; file.exists(chain_file) ; file.exists(input_bed)
```

```{r}
cmd <- glue('sbatch {liftover_sh} {liftover_exe} {chain_file} {input_bed} {output_bed} {unmapped_bed}')
cmd
```

```{r}
system(cmd)
```

- read in, merge and save

```{r}
hg19_bed <- data.table::fread(input_bed, col.names=c('chr', 'start.hg19', 'end.19', 'id'))
hg38_bed <- data.table::fread(output_bed, col.names=c('chr', 'start.hg38', 'end.hg39', 'id'))
bed_mappings <- dplyr::inner_join(hg19_bed, hg38_bed, by=c('chr' = 'chr', 'id' = 'id')) %>%
    dplyr::select(-id) %>% 
    dplyr::rename(chrom = chr)

data.table::fwrite(bed_mappings, file=glue('{data_dir}/files/baca_cwas_arbs_mappings.txt'), col.names=T, row.names=F, quote=F, sep='\t')
```


### SNP mappings i.e. liftover the snps' coordinates from hg19 to hg38

```{r}
# write out the cwas database snps
cwas_db %>%
    dplyr::select(chrom = chr, start = position, locus, rsid=snp_id, a1, a2) %>%
    dplyr::mutate(chrom = paste0('chr', chrom, sep=''), end = start + 1) %>%
    dplyr::relocate(end, .after = start) %>%
    data.table::fwrite(file=glue('{data_dir}/files/baca_cwas_snps_hg19.bed'), col.names=F, row.names=F, quote=F, sep='\t')

```

```{r}
liftover_sh <-  file.path(data_dir, 'src', 'liftoverSingleBed.sbatch')
liftover_exe <- file.path(data_dir, 'software', 'liftOver')
input_bed <- glue('{data_dir}/files/baca_cwas_snps_hg19.bed')
chain_file <- file.path(data_dir, 'helpers', 'hg19ToHg38.over.chain.gz')
output_bed <- glue('{data_dir}/files/baca_cwas_snps_hg38.bed')
unmapped_bed <- glue('{data_dir}/files/baca_cwas_snps_hg19.unmapped.bed')

file.exists(liftover_sh) ; file.exists(liftover_exe) ; file.exists(chain_file) ; file.exists(input_bed)
```

```{r}
cmd <- glue('sbatch {liftover_sh} {liftover_exe} {chain_file} {input_bed} {output_bed} {unmapped_bed}')
cmd
```

```{r}
system(cmd)
```

- read in, merge and save

```{r}
hg19_bed <- data.table::fread(input_bed, col.names=c('chrom', 'start.hg19', 'end.19', 'arbs.hg19', 'rsid', 'a1', 'a2'))
hg38_bed <- data.table::fread(output_bed, col.names=c('chr', 'start.hg38', 'end.hg39', 'arbs.hg19', 'rsid', 'a1', 'a2'))
snp_mappings <- dplyr::inner_join(hg19_bed, hg38_bed, by = c('arbs.hg19' = 'arbs.hg19', 'rsid' = 'rsid', 'a1' = 'a1', 'a2' = 'a2'))
data.table::fwrite(snp_mappings, file=glue('{data_dir}/files/baca_cwas_snp_mappings.txt'), col.names=T, row.names=F, quote=F, sep='\t')
```

-- Now, you can write out the db and save

```{r}
baca_models <- c('lasso', 'lasso.as', 'lasso.plasma', 'top1.as', 'top1.qtl', 'top1')
db_folder <- glue('{data_dir}/models/cwas/db_folder')
if(!dir.exists(db_folder)){dir.create(db_folder)}

db_folder_chr <- glue('{data_dir}/models/cwas/db_folder_chr')
if(!dir.exists(db_folder_chr)){dir.create(db_folder_chr)}
```

### You have to save in predictDB format

```{r}
snp_mappings <- data.table::fread(glue('{data_dir}/files/baca_cwas_snp_mappings.txt'))
weights_dt <- data.table::fread(file.path(data_dir, 'files', 'AR_baca_cwas_weights.hg19.2024-04-17.txt.gz'))
weights_dt <- dplyr::inner_join(weights_dt, snp_mappings, by=c('locus' = 'arbs.hg19', 'snp_id' = 'rsid', 'a1' = 'a1', 'a2' = 'a2')) %>%
    dplyr::select(rsid = snp_id, locus, chrom, position = start.hg38, a1, a2, all_of(baca_models))
head(weights_dt)
```

```{r}
# next, map the locus to hg38
bed_mappings <- data.table::fread(glue('{data_dir}/files/baca_cwas_arbs_mappings.txt')) %>%
    dplyr::mutate(locus.hg19 = paste0(chrom, ':', start.hg19, '-', end.19),
        locus.hg38 = paste0(chrom, '_', start.hg38, '_', end.hg39)) %>%
        dplyr::select(locus.hg19, locus.hg38)

weights_dt <- dplyr::inner_join(weights_dt, bed_mappings, by=c('locus' = 'locus.hg19')) %>%
    dplyr::select(-locus) %>%
    dplyr::rename(locus = locus.hg38) %>%
    dplyr::relocate(locus, .before = rsid)

head(weights_dt)
```

```{r}
# read in and map the extras

baca_extra <- data.table::fread(glue('{files_dir}/{transcription_factor}_baca_cwas_extras.hg19.{tdate}.txt.gz'))
baca_extra <- dplyr::inner_join(baca_extra, bed_mappings, by=c('locus' = 'locus.hg19')) %>%
    dplyr::select(-locus) %>%
    dplyr::rename(locus = locus.hg38) %>%
    dplyr::relocate(locus, .before = measure)
baca_extra$pred.perf.qval <- NA
baca_extra[1:5, ]
```

```{r}
baca_extra %>% dplyr::select(locus, measure, transcription_factor, n_snps_in_window, n.snps.in.model, pred.perf.qval, as.symbol('lasso')) %>% 
    tidyr::pivot_wider(names_from = 'measure', values_from = 'lasso') %>% 
    dplyr::relocate(c(pval, rsq), .after = locus)
```

```{r}
weights_dt %>% 
    dplyr::mutate(varID = paste0(chrom, '_', position, '_', a1, '_', a2, sep=''), gene = locus) %>%
    dplyr::select(gene, rsid, varID, ref_allele=a1, eff_allele=a2, weight=all_of('lasso')) %>%
    dplyr::mutate(varID = gsub("chr", '', varID))
```

```{r}
baca_weights_list <- purrr::map(.x=baca_models, function(each_m){
    model_weights <- weights_dt %>% 
        dplyr::mutate(varID = paste0(chrom, '_', position, '_', a1, '_', a2, sep=''), gene = locus) %>%
        dplyr::select(gene, rsid, varID, ref_allele=a1, eff_allele=a2, weight=all_of(each_m)) %>% 
        dplyr::mutate(varID = gsub("chr", '', varID))

    ## hg38
    edt <- baca_extra %>% 
        dplyr::select(locus, measure, transcription_factor, n_snps_in_window, n.snps.in.model, pred.perf.qval, all_of(each_m)) %>% 
        tidyr::pivot_wider(names_from = 'measure', values_from = each_m) %>% 
        dplyr::relocate(c(pval, rsq), .after = locus) %>%
        dplyr::filter(locus %in% weights_dt$locus) %>%
        dplyr::rename(gene = locus, genename = transcription_factor, pred.perf.R2 = rsq, pred.perf.pval = pval)

    each_db <- DBI::dbConnect(RSQLite::SQLite(), glue('{db_folder}/baca_cwas_{each_m}.hg38.db'))
    dbWriteTable(each_db, "extra", edt, overwrite=T)
    dbWriteTable(each_db, "weights", model_weights, overwrite=T)
    dbDisconnect(each_db)

    return(0)
})

# names(baca_weights_list) <- baca_models
```
-- see an example
```{r}
# '/project2/haky/temi/projects/TFXcan/baca_cwas/db_folder/baca_cwas_lasso.db'
mydb <- dbConnect(SQLite(), glue('{db_folder}/baca_cwas_lasso.hg38.db'))
ex <- dbGetQuery(mydb, 'SELECT * FROM extra')
wt <- dbGetQuery(mydb, 'SELECT * FROM weights')
ex |> head(); wt |> head()

dbDisconnect(mydb)
```

# Predict CWAS scores using these models

```{r}
db_folder <- '/project/haky/users/temi/projects/Enpact/models/cwas/db_folder'
txt_genotypes <- '/project2/haky/Data/baca_cwas/vcfs/hg38/formatted_geno/all_chrs.text_dosages.txt.gz'
txt_samples <- '/project2/haky/Data/baca_cwas/vcfs/hg38/formatted_geno/samples.text_dosages.txt'
exec_file <- '/beagle3/haky/users/temi/software/MetaXcan/software/Predict.py'
output_folder <- "/project/haky/users/temi/projects/Enpact/data/baca_cwas/output"
if(!dir.exists(output_folder)){dir.create(output_folder, recursive = T)}
```

```{r}
# if needed, you should edit the sbatch file: {data_dir}/src/predictCWASscores.sbatch
cmd <- glue('sbatch {data_dir}/src/predictCWASscores.sbatch {db_folder} {txt_genotypes} {txt_samples} {exec_file} {output_folder}')
cmd
```

```{r}
system(cmd) ; system('squeue -u temi')
```

- prepare the file in a simple matrix

```{r}
# read in the individual mappings
samples_metadata <- data.table::fread(file.path(data_dir, 'metadata', 'baca_samples_mappings.metadata.txt'))

cwas_scores <- data.table::fread(glue('{output_folder}/top1.hg38/baca_cwas_predict.txt')) %>%
    dplyr::select(-FID) %>%
    tibble::column_to_rownames('IID') %>%
    t() %>% as.data.frame()

cwas_mat <- cwas_scores[,]

colnames(cwas_mat) <- gsub('UW_PDX_172', 'UW_PDX_170_2', colnames(cwas_mat))
colnames(cwas_mat) <- gsub('UW_PDX_173', 'UW_PDX_170_3', colnames(cwas_mat))
colnames(cwas_mat) <- samples_metadata[match(colnames(cwas_mat), samples_metadata$vcf), ]$id
cwas_mat <- cwas_mat %>% tibble::rownames_to_column('locus')
```

```{r}
data.table::fwrite(cwas_mat, file=glue('{data_dir}/files/baca_cwas_scores.hg38.txt.gz'), col.names=T, row.names=F, quote=F, compress='gzip', sep='\t')
```

# Predict Enpact scores using the DL-based Enpact model
The steps here are involved. 

1. Get epigenomic features for all Baca individuals with Enformer. In this paper, we used [this pipeline]().
2. With the DL-based Enpact model, predict the Enpact scores using [this pipeline](https://github.com/hakyimlab/enpact-predict-snakemake)

# Train a SNP-based Enpact model

1. Get epigenomic features across 521 EUR individuals with Enformer; In this paper, we used [this pipeline]().
2. With the DL-based Enpact model, predict the Enpact scores using [this pipeline](https://github.com/hakyimlab/enpact-predict-snakemake)
3. Prepare the matrix and data for the SNP-based Enpact model such that they are compatible with predictDB format
4. Train the SNP-based Enpact model using [this pipeline]()

# Predict Enpact scores using the SNP-based Enpact model

```{r}
db_name <- 'EUR_AR_Prostate_logistic'
model_db <- '/project/haky/users/temi/projects/Enpact/models/lenpact/predict_db_EUR_AR_Prostate_logistic_filtered.db'
txt_genotypes <- '/project2/haky/Data/baca_cwas/vcfs/hg38/formatted_geno/all_chrs.text_dosages.txt.gz'
txt_samples <- '/project2/haky/Data/baca_cwas/vcfs/hg38/formatted_geno/samples.text_dosages.txt'
exec_file <- '/beagle3/haky/users/temi/software/MetaXcan/software/Predict.py'
output_folder <- "/project/haky/users/temi/projects/Enpact/data/baca_lenpact/output"
if(!dir.exists(output_folder)){dir.create(output_folder, recursive = T)}
```

```{r}
# if needed, you should edit the sbatch file: {data_dir}/src/predictCWASscores.sbatch
cmd <- glue('sbatch {data_dir}/src/predictENPACTscores.sbatch {db_name} {model_db} {txt_genotypes} {txt_samples} {exec_file} {output_folder}')
cmd
```

```{r}
system(cmd) ; system('squeue -u temi')
```

# Running TFXcan on prostate cancer data
You can download the GWAS summary statistic from the CWAS paper repo [here](https://github.com/scbaca/cwas/blob/master/gwas_data/ProstateCancer_Meta_Schumacher2018.nodup.sumstats.gz) or, better still, from the [GWAS catalog](https://www.ebi.ac.uk/gwas/publications/29892016). This is [the direct link](https://ftp.ebi.ac.uk/pub/databases/gwas/summary_statistics/GCST006001-GCST007000/GCST006085/harmonised/29892016-GCST006085-EFO_0001663.h.tsv.gz)

-- If you have/want to download from the GWAS catalog, this following scripts should help with processing
```{r}
gwas_data <- file.path(data_dir, 'data', 'sumstats', '29892016-GCST006085-EFO_0001663.h.tsv.gz')
dir.create(dirname(gwas_data), recursive = F)
if(!file.exists(gwas_data)){
    download.file('https://ftp.ebi.ac.uk/pub/databases/gwas/summary_statistics/GCST006001-GCST007000/GCST006085/harmonised/29892016-GCST006085-EFO_0001663.h.tsv.gz', destfile = gwas_data)
}
```

-- process the downloaded file. I have processed this file in accordance with the weights data from the SNP-based Enpact model. So, you will need to read in the models and use the data to filter for compatible SNPs.

```{r}
pcr_sumstat <- data.table::fread(gwas_data)
dim(pcr_sumstat); pcr_sumstat[1:5, 1:10]
```
-- read in the lEnpact weights
```{r}
# model_db <- file.path(data_dir, 'models', 'lenpact', 'predict_db_EUR_AR_Prostate_logistic_filtered.db')
enpactdb <- dbConnect(SQLite(), model_db)
weights <- dbGetQuery(enpactdb, 'SELECT * FROM weights')
dbDisconnect(enpactdb)
```

reformat the file and filter for only those with variant Id in the weights data
```{r}
pcgss <- pcr_sumstat %>% 
    dplyr::select(chrom=hm_chrom, variant_id=hm_variant_id, rsid=hm_rsid, pos=hm_pos, A2=hm_other_allele, A1=hm_effect_allele, beta=hm_beta, p_value=p_value, se=standard_error, maf=hm_effect_allele_frequency) %>% 
    dplyr::filter(!(is.na(variant_id) | is.na(rsid) | is.na(pos))) %>%
    dplyr::mutate(zscore=beta/se) %>% 
    dplyr::filter(variant_id %in% weights$varID)

pcgss[1:5, 1:5]; dim(pcgss)
```

```{r}
output_folder <- file.path(data_dir, 'data', 'sumstats')
if(!dir.exists(output_folder)){
    dir.create(output_folder, recursive=T)
}

pcgss %>% split(.$chrom) %>% imap(~data.table::fwrite(.x, glue('{output_folder}/chr{.y}_Schumacher.gwas_ss.txt.gz'), compress='gzip', row.names=F, quote=F, sep = '\t'))
```

-- Now, you can run TFXcan on the prostate cancer data

```{r}
db_name <- 'EUR_AR_Prostate_logistic'
model_db <- file.path(data_dir, 'models/lenpact/predict_db_EUR_AR_Prostate_logistic_filtered.db')
covariances <- file.path(data_dir, 'models/lenpact/Covariances.varID.txt')
gwas_folder <- file.path(data_dir, 'data/sumstats')
gwas_file_pattern <- ".*_Schumacher.gwas_ss.txt.gz"
exec_file <- '/beagle3/haky/users/temi/software/MetaXcan/software/SPrediXcan.py'
output_file <- file.path(data_dir, 'data', 'tfxcan', 'EUR_AR_Prostate_logistic.TFXcan.prostate_cancer_risk.csv')
if(!dir.exists(dirname(output_file))){dir.create(dirname(output_file), recursive = T)}
```

```{r}
cmd <- glue('sbatch {data_dir}/src/sTFXcan.sbatch {db_name} {model_db} {covariances} {gwas_folder} {gwas_file_pattern} {exec_file} {output_file}')
cmd
```

```{r}
system(cmd) ; system('squeue -u temi')
```

# Running TWAS on prostate cancer data

This is very similar to using the PrediXcan pipeline. You will need to download the TWAS weights from the [PrediXcan website](https://predictdb.org/). You can use the [GTEx v8 weights](https://predictdb.org/download/weights/GTEx_V8_HapMap-2017-11-29.tar.gz) or the [GTEx v7 weights](https://predictdb.org/download/weights/GTEx_V7_HapMap-2017-11-29.tar.gz).


```{r}
knitr::knit_exit()
```


```{r}

```
```{r}
chainObject <- rtracklayer::import.chain(file.path(data_dir, 'helpers', 'hg19ToHg38.over.chain'))
```

```{r}
grObject <- GenomicRanges::GRanges(seqnames = cwas_db$chr, ranges = IRanges(start = cwas_db$position, end = cwas_db$position))
results <- as.data.frame(liftOver(grObject, chainObject))
dim(results)
```

- you lose some 422 loci
```{r}
nrow(cwas_db) - nrow(results)
```

- liftover to hg38
```{r}
cvperf_dt_split <- cvperf_dt %>%
    tidyr::separate_wider_delim(col = locus, names = c('chr', 'start', 'end'), delim = stringr::regex(':|-')) %>%
    dplyr::mutate(across(c(start, end), as.numeric)) %>%
    dplyr::select(chr, start, end) %>%
    dplyr::distinct() %>%
    dplyr::mutate(id = 1:nrow(.))

cvperf_granges <- with(cvperf_dt_split, GenomicRanges::GRanges(seqnames = chr, ranges = IRanges(start = start, end = end), id= id))
lfobject <- liftOver(cvperf_granges, chainObject)
# lfobject <- reduce(lfobject, min.gapwidth = 20L)
cvperf_liftover <- as.data.frame(lfobject)
dim(cvperf_liftover)
```

- merge duplicated granges
```{r}
cvperf_liftover <- purrr::map2(list(min, max), list('start', 'end'), ~ 
      cvperf_liftover %>%
         group_by(id) %>%
         summarise_at(.y, .x)) %>%
         purrr::reduce(inner_join, by = 'id') %>%
         dplyr::rename(start.hg38 = start, end.hg38 = end)

cvperf_liftover[1:5, ]
```

```{r}
cwas_bed_mappings <- cvperf_dt_split %>% dplyr::rename(start.hg19 = start, end.hg19 = end) %>%
    dplyr::inner_join(cvperf_liftover, by = 'id') %>%
    dplyr::mutate(cwas_locus.hg19 = paste0(chr, ':', start.hg19, '-', end.hg19),
        cwas_locus.hg38 = paste0(chr, '_', start.hg38, '_', end.hg38))
```

```{r}
hg38bedfiles <- hg38_bed_files %>%
    dplyr::mutate(hg38_id = paste0(chr, '_', hg38_start, '_', hg38_end))
```

```{r}
which(!cwas_bed_mappings$cwas_locus.hg38 %in% hg38bedfiles$hg38_id); which(!hg38bedfiles$hg38_id %in% cwas_bed_mappings$cwas_locus.hg38)
```

```{r}
hg38bedfiles[186, ] ; cwas_bed_mappings %>% dplyr::filter(cwas_locus.hg19 == 'chr1:145543400-145544050')
```


```{r}
length(cvperf_liftover$id |> unique()) - length(cvperf_dt_split$id)
```

```{r}
cvperf_liftover %>% 
  group_by(id) %>% 
  filter(n()>1)
```

```{r}
cvperf_liftover %>%
    dplyr::group_by(id) %>%
    filter(start == min(start), end == max(end)) %>%
    filter(n()>1)
```

- split into models and save 
```{r}
dt <- cvperf_dt %>% 
    tidyr::pivot_longer(cols = !c(locus, measure, transcription_factor, n_snps_in_window, n.snps.in.model), values_to = 'score', names_to=c('model')) %>% 
    tidyr::pivot_wider(id_cols = c(locus, model, transcription_factor, n_snps_in_window, n.snps.in.model), names_from = 'measure', values_from = 'score') %>%
    dplyr::mutate(locus = gsub(':|-', '_', locus)) %>%
    dplyr::full_join(loci_mappings, by = c('locus' = 'hg19_id')) %>%
    dplyr::rename(hg19_id = locus, pred.perf.R2 = rsq, pred.perf.pval = pval)

dt %>% dplyr::group_by(model) %>%
    group_walk(~ data.table::fwrite(.x, file=glue('{files_dir}/{transcription_factor}_baca_cwas_{tdate}.{.y$model}.cv_performance.txt.gz'), col.names=T, row.names=F, quote=F, compress='gzip',sep = '\t'), .keep = TRUE)
```


```{r}
cwas_db <- cwas_db %>%
    dplyr::mutate(varID = gsub(":|-", '_', locus))
extra_dt <- extra_dt %>%
    dplyr::mutate(varID = gsub(":|-", '_', locus))
```

```{r}
bedmappings <- glue('{base_dir}/mappings/baca_cwas_loci_hg38.bed')
snpmappings <- glue('{base_dir}/mappings/hg38_snps.bed')
```

```{r}
hg38_snp_files <- data.table::fread(snpmappings, col.names=c('chr', 'hg38_start', 'hg38_end', 'snp_id', 'hg19_locus'))
hg38_snp_files$chr <- as.numeric(gsub(pattern='chr', replacement='', x=hg38_snp_files$chr))
hg38_snp_files[1:5, ]
```

```{r}
tt <- dplyr::left_join(hg38_cwas_db, hg38_snp_files, by=c('locus' = 'hg38_locus', 'snp_id'='snp_id', 'chr' = 'chr'))
tt$position <- tt$hg38_start
tt$hg38_start <- tt$hg38_end <- NULL
tt[1:5, ] ; dim(tt)
```

```{r}
mappings <- data.table::fread('/project2/haky/temi/projects/TFXcan/baca_cwas/mappings/baca_cwas_loci_hg38.bed', col.names=c('chr', 'hg38_start', 'hg38_end', 'hg19_id'))

mappings <- mappings %>%
    tidyr::unite('hg38_id', chr:hg38_end, sep = '_', remove = T) %>%
    dplyr::mutate(hg19_id = gsub(':|-', '_', hg19_id))

data.table::fwrite(mappings, file = '/project2/haky/temi/projects/TFXcan/baca_cwas/mappings/cwas_loci_mappings.txt', sep = '\t', col.names = T, row.names = F, quote = F)
```

loci_mappings <- data.table::fread('/project2/haky/temi/projects/TFXcan/baca_cwas/mappings/cwas_loci_mappings.txt')
data.table::fwrite(extra_dt, file=glue('{files_dir}/{transcription_factor}_baca_cwas_extras_{tdate}.hg19.txt.gz'), row.names=F, quote=F, compress='gzip', sep='\t')
```

#### Create db (in hg19)
Baca has 6 models/weights

```{r}
baca_models <- c('lasso', 'lasso.as', 'lasso.plasma', 'top1.as', 'top1.qtl', 'top1')
db_folder <- glue('{base_dir}/db_folder')
if(!dir.exists(db_folder)){dir.create(db_folder)}

db_folder_chr <- glue('{base_dir}/db_folder_chr')
if(!dir.exists(db_folder_chr)){dir.create(db_folder_chr)}
```

```{r}
baca_extra <- data.table::fread(glue('{files_dir}/{transcription_factor}_baca_cwas_extras_{tdate}.hg19.txt.gz'))
baca_extra <- baca_extra %>% 
    dplyr::mutate(gene = gsub(":|-", '_', locus)) %>%
    dplyr::select(-c(varID)) %>%
    dplyr::rename(genename=transcription_factor)
baca_extra$pred.perf.qval <- NA
baca_extra[1:5, ]
```

Predict_db format

```{r}

weights_dt <- data.table::fread(glue('{files_dir}/{transcription_factor}_baca_cwas_weights_{tdate}.hg19.txt.gz'))

baca_weights_list <- purrr::map(.x=baca_models, function(each_m){
    model_weights <- weights_dt %>% 
        dplyr::mutate(varID = paste0(chr, '_', position, '_', a1, '_', a2, sep=''),
            chr_varID = paste0('chr', chr, '_', position, '_', a1, '_', a2, sep=''),
            gene = gsub(":|-", '_', locus)) %>%
        dplyr::select(gene, rsid=snp_id, varID, chr_varID, ref_allele=a1, eff_allele=a2, weight=as.symbol(each_m)) %>% as.data.frame()

    # read in the extra
    extras_dt <- data.table::fread(glue('{files_dir}/{transcription_factor}_baca_cwas_{tdate}.{each_m}.cv_performance.txt.gz'))

    # prepare for both hg19 and hg38

    ## hg19 
    each_db <- DBI::dbConnect(RSQLite::SQLite(), glue('{db_folder}/baca_cwas_{each_m}.hg19.db'))
    dbWriteTable(each_db, "extra", extras_dt %>% dplyr::rename(gene = hg19_id), overwrite=T)
    dbWriteTable(each_db, "weights", model_weights, overwrite=T)
    dbDisconnect(each_db)

    ## hg38
    edt <- extras_dt %>%
        dplyr::select(hg19_id, hg38_id)
    model_wgts <- dplyr::full_join(model_weights, edt, by = c('gene' = 'hg19_id')) %>%
        dplyr::select(-any_of(c(gene, hg19_id))) %>%
        dplyr::rename(gene = hg38_id)


    each_db <- DBI::dbConnect(RSQLite::SQLite(), glue('{db_folder}/baca_cwas_{each_m}.hg19.db'))
    dbWriteTable(each_db, "extra", baca_extra, overwrite=T)
    dbWriteTable(each_db, "weights", model_weights, overwrite=T)
    dbDisconnect(each_db)

    # model_weights <- baca_weights %>% 
    #     dplyr::select(gene=locus, rsid=snp_id, varID=chr_varIDs, chr_varIDs=chr_varIDs, ref_allele=a1, eff_allele=a2, weight=as.symbol(each_m)) %>% as.data.frame()

    # each_db <- DBI::dbConnect(RSQLite::SQLite(), glue('{db_folder_chr}/baca_cwas_{each_m}.db'))
    # dbWriteTable(each_db, "extra", baca_extra, overwrite=T)
    # dbWriteTable(each_db, "weights", model_weights, overwrite=T)
    # dbDisconnect(each_db)

    return(0)
})

# names(baca_weights_list) <- baca_models
```

### Read in the cv performance

```{r}
out <- purrr::map(.x=seq_along(ar_files_locus), .f=function(i){
    locus <- ar_files_locus[i]
    #print(file.exists(glue('{output_dir}/{transcription_factor}/{locus}.wgt.RDat')))
    rdt <- new.env(parent = emptyenv())
    load(glue('{output_dir}/{transcription_factor}/{locus}.wgt.RDat'), envir=rdt)

    cv_perf <- rbind(rdt$cv.performance['pval', ], rdt$cv.performance['rsq', ]) %>%
        as.data.frame() %>%
        dplyr::mutate(measure = c('pval', 'rsq'), snp_id = locus) %>%
        dplyr::relocate(snp_id, measure)

    return(cv_perf)
}, .progress=T)

cvperf_dt <- do.call('rbind', out)
cvperf_dt <- cvperf_dt %>% dplyr::rename(locus = snp_id)

dim(cvperf_dt) ; cvperf_dt[1:5, ]
```

```{r}
dt <- dplyr::left_join(cvperf_dt, hg38_bed_files, by=c('locus' = 'hg19_id')) %>%
    dplyr::mutate(locus = hg38_id) %>%
    dplyr::rename(chrom = chr) %>%
    dplyr::relocate(chrom, locus, measure) %>%
    dplyr::select(-hg38_id) %>%
    dplyr::filter(!is.na(locus))
```

```{r}
sum(is.na(dt$locus))
```

```{r}
dt %>% 
    tidyr::pivot_longer(cols = !c(chrom, locus, measure), values_to = 'score', names_to=c('model')) %>% 
    tidyr::pivot_wider(id_cols =c(locus, model),  names_from = 'measure', values_from = 'score') %>%
    dplyr::group_by(model) %>%
    group_walk(~ data.table::fwrite(.x, file=glue('{files_dir}/{transcription_factor}_baca_cwas_hg38_{tdate}.{.y$model}.cv_performance.txt.gz'), col.names=T, row.names=F, quote=F, compress='gzip',sep = '\t'), .keep = TRUE)
```

```{r}
dtgrp[[1]] %>% dplyr::group_by(locus, model, measure) %>%
  dplyr::summarise(n = dplyr::n(), .groups = "drop") %>%
  dplyr::filter(n > 1L) 
```

```{r}
dt <- dplyr::left_join(cvperf_dt, hg38_bed_files, by=c('locus' = 'hg19_id')) %>% 
    dplyr::mutate(locus = hg38_id) %>%
    dplyr::rename(chrom = chr) %>%
    dplyr::relocate(chrom, locus, measure) %>%
    dplyr::select(-hg38_id)
data.table::fwrite(dt, file=glue('{files_dir}/{transcription_factor}_baca_cwas_hg38_{tdate}.cv_performance.txt.gz'), col.names=T, row.names=F, quote=F, compress='gzip',sep = '\t')
```

```{r}
dt <- data.table::fread(glue('{files_dir}/{transcription_factor}_baca_cwas_hg38_{tdate}.cv_performance.txt.gz'))
```

```{r}
pval_mat <- dt %>%
    dplyr::filter(measure == 'pval') %>%
    dplyr::select(-c(chrom, locus, measure)) %>%
    as.matrix() 

rsq_mat <- dt %>%
    dplyr::filter(measure == 'rsq') %>%
    dplyr::select(-c(chrom, locus, measure)) %>%
    as.matrix() 
```

```{r}
# Function to add histograms
panel.hist <- function(x, ...) {
    # usr <- par("usr")
    # on.exit(par(usr))
    # par(usr = c(usr[1:2], 0, 1.5))
    par(new = TRUE)
    his <- hist(x, plot = T, main = '')
    # breaks <- his$breaks
    # nB <- length(breaks)
    # y <- his$counts
    # y <- y/max(y)
    # rect(breaks[-nB], 0, breaks[-1], y, col = rgb(0, 1, 1, alpha = 0.5), ...)
    # lines(density(x), col = 2, lwd = 2) # Uncomment to add density lines
}

# panel.qqplot <- function(x, ...) {
#     usr <- par("usr")
#     on.exit(par(usr))
#     par(usr = c(usr[1:2], 0, 1.5))
#     qq_generic(data_points = x, distribution = 'uniform')
#     # lines(density(x), col = 2, lwd = 2) # Uncomment to add density lines
# }


panel.cor <- function(x,y, ...){
    par(new = TRUE)
    plot(x, y)
    abline(a=0, b=1, col='red')
    # legend('bottomright', legend=c('y', 'x'), col=c("black", "blue"), pch=c(1,3), bg = adjustcolor('grey', 0.2))
}

panel.qqplot <- function(x,y, ...){
    par(new = TRUE)
    qqunif.compare(x, y)
    legend('bottomright', legend=c('y', 'x'), col=c("black", "blue"), pch=c(1,3), bg = adjustcolor('grey', 0.2))
}
```



```{r}
qqunif(pval_mat[, 'lasso'], col = 'orange', pch = '.', cex = 3, bty = 'n', show=F)
qqpoints(pval_mat[, 'lasso.as'], col = 'brown', pch = '.', cex = 3)
qqpoints(pval_mat[, 'top1'], col = 'red', pch = '.', cex = 3)
qqpoints(pval_mat[, 'lasso.plasma'], col = 'blue', pch = '.', cex = 3)
qqpoints(pval_mat[, 'top1.as'], col = 'green', pch = '.', cex = 3)
qqpoints(pval_mat[, 'top1.qtl'], col = 'black', pch = '.', cex = 3)
legend(x=0, y =20, legend = c('lasso', 'lasso.as', 'top1', 'lasso.plasma', 'top1.as', 'top1.qtl'), pch = '.', pt.cex=3, col=c('orange', 'brown', 'red', 'blue', 'green', 'black'), bty='n')

```

```{r}
# pdf(glue('{files_dir}/plt.pdf'), width=21, height=21)
# pairs(pval_mat, lower.panel = panel.qqplot, diag.panel = panel.hist, upper.panel = NULL, gap=3)
# dev.off()

pdf(glue('{files_dir}/rsq_plt.pdf'), width=21, height=21)
pairs(rsq_mat, lower.panel = panel.cor, diag.panel = panel.hist, upper.panel = NULL, gap=3)
dev.off()

```

```{r}
qqplot(pval_mat[1:100, 1], pval_mat[1:100, 2])
```

if you have a lifted over bed file, you can continue
Read in new bed files and match 
```{r}
hg38_bed_files <- data.table::fread(bedmappings, col.names=c('chr', 'hg38_start', 'hg38_end', 'hg19_id'))
hg38_bed_files <- hg38_bed_files %>% 
    dplyr::mutate(hg38_id=paste(paste(chr, hg38_start, sep=':'), hg38_end, sep='-')) %>%
    dplyr::select(chr, hg19_id, hg38_id)
hg38_bed_files$chr <- as.numeric(gsub(pattern='chr', replacement='', x=hg38_bed_files$chr))
hg38_bed_files[1:5, ]
```

```{r}
hg38_cwas_db <- dplyr::left_join(cwas_db, hg38_bed_files, by=c('locus' = 'hg19_id', 'chr'='chr'))
hg38_cwas_db$locus <- hg38_cwas_db$hg38_id
hg38_cwas_db$hg38_id <- NULL
hg38_cwas_db[1:5, ] ; dim(hg38_cwas_db)
```

Match the `extras` too

```{r}
hg38_cwas_extra <- dplyr::left_join(extra_dt, hg38_bed_files, by=c('locus' = 'hg19_id'))
hg38_cwas_extra$locus <- hg38_cwas_extra$hg38_id
hg38_cwas_extra$hg38_id <- NULL
hg38_cwas_extra[1:5, ] ; dim(hg38_cwas_extra)
```

Merge with the hg38 snps
```{r}
hg38_snp_files <- data.table::fread(snpmappings, col.names=c('chr', 'hg38_start', 'hg38_end', 'snp_id', 'hg38_locus'))
hg38_snp_files$chr <- as.numeric(gsub(pattern='chr', replacement='', x=hg38_snp_files$chr))
hg38_snp_files[1:5, ]
```

```{r}
tt <- dplyr::left_join(hg38_cwas_db, hg38_snp_files, by=c('locus' = 'hg38_locus', 'snp_id'='snp_id', 'chr' = 'chr'))
tt$position <- tt$hg38_start
tt$hg38_start <- tt$hg38_end <- NULL
tt[1:5, ] ; dim(tt)
```

```{r}
# write out the snps to a file
data.table::fwrite(tt, file=glue('{files_dir}/{transcription_factor}_baca_cwas_weights_hg38_{tdate}.txt.gz'), col.names=T, row.names=F, quote=F, compress='gzip',sep = '\t')

data.table::fwrite(hg38_cwas_extra, file=glue('{files_dir}/{transcription_factor}_baca_cwas_extra_hg38_{tdate}.txt.gz'), col.names=T, row.names=F, quote=F, compress='gzip',sep = '\t')
```


## Create databases

- for the weights as is
Here I will write out two folders: one with `1...` and `chr1...`
```{r}
library(RSQLite)
library(dbplyr)
library(DBI)
```

```{r}
db_folder <- glue('{base_dir}/db_folder')
if(!dir.exists(db_folder)){dir.create(db_folder)}

db_folder_chr <- glue('{base_dir}/db_folder_chr')
if(!dir.exists(db_folder_chr)){dir.create(db_folder_chr)}
```


```{r}
# write out the snps to a file
baca_weights <- data.table::fread(glue('{files_dir}/{transcription_factor}_baca_cwas_weights_hg38_{tdate}.txt.gz'))
baca_weights$varIDs <- with(baca_weights, paste0(chr, '_', position, '_', a1, '_', a2, sep=''))
baca_weights$chr_varIDs <- with(baca_weights, paste0('chr', chr, '_', position, '_', a1, '_', a2, sep=''))
baca_weights[1:5, ]
```

Baca has 6 models/weights

```{r}
baca_models <- c('lasso', 'lasso.as', 'lasso.plasma', 'top1.as', 'top1.qtl', 'top1')
```

```{r}
baca_extra <- data.table::fread(glue('{files_dir}/{transcription_factor}_baca_cwas_extra_hg38_{tdate}.txt.gz'))
baca_extra <- baca_extra %>% dplyr::rename(gene=locus, genename=transcription_factor)
baca_extra$pred.perf.qval <- NA
baca_extra[1:5, ]
```

Predict_db format

```{r}
baca_weights_list <- purrr::map(.x=baca_models, function(each_m){
    model_weights <- baca_weights %>% 
        dplyr::select(gene=locus, rsid=snp_id, varID=varIDs, chr_varID=chr_varIDs, ref_allele=a1, eff_allele=a2, weight=as.symbol(each_m)) %>% as.data.frame()

    each_db <- DBI::dbConnect(RSQLite::SQLite(), glue('{db_folder}/baca_cwas_{each_m}.db'))
    dbWriteTable(each_db, "extra", baca_extra, overwrite=T)
    dbWriteTable(each_db, "weights", model_weights, overwrite=T)
    dbDisconnect(each_db)

    # model_weights <- baca_weights %>% 
    #     dplyr::select(gene=locus, rsid=snp_id, varID=chr_varIDs, chr_varIDs=chr_varIDs, ref_allele=a1, eff_allele=a2, weight=as.symbol(each_m)) %>% as.data.frame()

    # each_db <- DBI::dbConnect(RSQLite::SQLite(), glue('{db_folder_chr}/baca_cwas_{each_m}.db'))
    # dbWriteTable(each_db, "extra", baca_extra, overwrite=T)
    # dbWriteTable(each_db, "weights", model_weights, overwrite=T)
    # dbDisconnect(each_db)

    return(0)
})

# names(baca_weights_list) <- baca_models
```




Now you can look at one of them...
```{r}
lasso_db <- DBI::dbConnect(RSQLite::SQLite(), glue('{db_folder}/baca_cwas_lasso.db'))
extra_dt <- tbl(lasso_db, 'extra') %>% as.data.frame()
weights_dt <- tbl(lasso_db, 'weights') %>% as.data.frame()

dbDisconnect(lasso_db)
```

```{r}
extra_dt |> head() ; weights_dt |> head()
```



```{r}
lasso_db <- DBI::dbConnect(RSQLite::SQLite(), glue('{db_folder_chr}/baca_cwas_lasso.db'))
extra_dt <- tbl(lasso_db, 'extra') %>% as.data.frame()
weights_dt <- tbl(lasso_db, 'weights') %>% as.data.frame()

dbDisconnect(lasso_db)
```

```{r}
extra_dt |> head() ; weights_dt |> head()
```



```{r}
zcat in.vcf.gz | vcftools_0.1.9/bin/vcf-annotate --fill-type | grep -oP "TYPE=\w+" | sort | uniq -c
```